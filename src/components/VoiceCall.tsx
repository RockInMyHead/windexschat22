import React, { useEffect, useRef, useState, useCallback } from 'react';
import { Mic, MicOff, Phone, PhoneOff, Volume2, VolumeX } from 'lucide-react';
import { Button } from './ui/button';
import { Card } from './ui/card';
import { cn } from '@/lib/utils';

interface VoiceCallProps {
  wsUrl?: string;
  onTranscript?: (text: string, isFinal: boolean) => void;
  onLLMResponse?: (delta: string, isStart: boolean, isEnd: boolean) => void;
  className?: string;
  autoStart?: boolean;
}

type ConnectionState = 'disconnected' | 'connecting' | 'connected' | 'error';
type CallState = 'idle' | 'active' | 'speaking' | 'listening';

export const VoiceCall: React.FC<VoiceCallProps> = ({
  wsUrl = 'ws://127.0.0.1:2700',
  onTranscript,
  onLLMResponse,
  className,
  autoStart = false
}) => {
  // Connection state
  const [connectionState, setConnectionState] = useState<ConnectionState>('disconnected');
  const [callState, setCallState] = useState<CallState>('idle');
  const [isMuted, setIsMuted] = useState(false);
  const [audioEnabled, setAudioEnabled] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // Transcript state
  const [partialTranscript, setPartialTranscript] = useState('');
  const [finalTranscript, setFinalTranscript] = useState('');
  const [llmResponse, setLlmResponse] = useState('');
  const [protocolVersion, setProtocolVersion] = useState(1);
  const backendReadyRef = useRef(false);

  // Refs
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const playbackAudioContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioQueueRef = useRef<Uint8Array[]>([]);
  const isPlayingRef = useRef(false);
  const isStartingRef = useRef(false);
  const isMutedRef = useRef(false);
  const isLLMRespondingRef = useRef(false);
  const ttsChunkCountRef = useRef(0);
  const isTTSActiveRef = useRef(false);

  // Sync ref with state
  useEffect(() => {
    isMutedRef.current = isMuted;
  }, [isMuted]);

  /**
   * Initialize WebSocket connection
   */
  const connectWebSocket = useCallback(() => {
    if (wsRef.current?.readyState === WebSocket.OPEN) return;

    setConnectionState('connecting');
    
    try {
      const ws = new WebSocket(wsUrl);
      ws.binaryType = 'arraybuffer';

      ws.onopen = () => {
        console.log('üîå WebSocket connected');
        setConnectionState('connected');
        
        // Send config
        ws.send(JSON.stringify({
          config: {
            sample_rate: 16000,
            words: false,
            protocol_version: 2
          }
        }));
      };

      ws.onmessage = async (event) => {
        if (event.data instanceof ArrayBuffer) {
          console.log(`üì¶ Received binary data: ${event.data.byteLength} bytes (TTS active: ${isTTSActiveRef.current}, chunk #${ttsChunkCountRef.current + 1})`);
          // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
          handleBinaryAudio(event.data);
          return;
        }

        try {
          const message = JSON.parse(event.data);
          // –ë—ç–∫–µ–Ω–¥ –º–æ–∂–µ—Ç –ø—Ä–∏—Å—ã–ª–∞—Ç—å —Ç–∏–ø –≤ –ø–æ–ª–µ 'type' –∏–ª–∏ 'event'
          const msgType = message.type || message.event;
          console.log(`üì® Received message: ${msgType}`, message);
          handleWebSocketMessage({ ...message, type: msgType });
        } catch (error) {
          console.error('Failed to parse message:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        setConnectionState('error');
      };

      ws.onclose = () => {
        console.log('üîå WebSocket disconnected');
        setConnectionState('disconnected');
        backendReadyRef.current = false;
        wsRef.current = null;
      };

      wsRef.current = ws;
    } catch (error) {
      console.error('Failed to connect WebSocket:', error);
      setConnectionState('error');
    }
  }, [wsUrl]);

  /**
   * Handle WebSocket messages
   */
  const handleWebSocketMessage = useCallback((message: any) => {
    switch (message.type) {
      case 'partial':
        setPartialTranscript(message.partial || '');
        onTranscript?.(message.partial || '', false);
        setCallState('listening');
        break;

      case 'final':
        if (message.text?.trim()) {
          setFinalTranscript(prev => prev + ' ' + message.text);
          onTranscript?.(message.text, true);
          setPartialTranscript('');
        }
        break;

      case 'nlu_start':
      case 'llm_start':
        if (!isLLMRespondingRef.current) {
          isLLMRespondingRef.current = true;
          setLlmResponse('');
          setCallState('speaking');
          onLLMResponse?.('', true, false); // isStart = true
        }
        break;

      case 'llm_delta':
        setLlmResponse(prev => prev + (message.delta || ''));
        onLLMResponse?.(message.delta || '', false, false); // delta chunk
        break;

      case 'nlu_end':
      case 'llm_end':
        isLLMRespondingRef.current = false;
        setCallState('active');
        onLLMResponse?.('', false, true); // isEnd = true
        break;

      case 'llm_error':
        isLLMRespondingRef.current = false;
        console.error('LLM error:', message.error);
        setCallState('active');
        break;

      case 'abort':
        console.log('üõë Abort received:', message.reason);
        audioQueueRef.current = [];
        isPlayingRef.current = false;
        setCallState('active');
        break;

        case 'tts_start':
          console.log('üîä TTS started');
          isTTSActiveRef.current = true;
          ttsChunkCountRef.current = 0;
          setCallState('speaking');
          break;

      case 'tts_end':
        console.log(`üîä TTS ended (received ${ttsChunkCountRef.current} audio chunks)`);
        isTTSActiveRef.current = false;
        // –ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ idle, —á—Ç–æ–±—ã –∑–≤–æ–Ω–æ–∫ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–ª—Å—è
        if (protocolVersion < 2) {
          setCallState('active');
        }
        break;

      case 'tts_error':
        console.error('TTS error:', message.error);
        break;

      case 'tts_audio':
        // –≠—Ç–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –±–∏–Ω–∞—Ä–Ω—ã–º –∞—É–¥–∏–æ-—á–∞–Ω–∫–æ–º
        // –°–ª–µ–¥—É—é—â–∏–π –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ—Ä–µ–π–º –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞—É–¥–∏–æ
        console.log(`üéµ TTS audio metadata: utterance_id=${message.utterance_id}, mime=${message.mime}`);
        // –ù–µ –Ω—É–∂–Ω–æ –Ω–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å, –ø—Ä–æ—Å—Ç–æ –∂–¥–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        break;

      case 'ready':
        console.log('‚úÖ Backend ready', message);
        if (message.protocol_version) {
          setProtocolVersion(message.protocol_version);
        }
        backendReadyRef.current = true;
        console.log('‚úÖ Ready received, can start sending PCM');
        break;

      case 'pong':
        // Keep-alive response
        break;

      case 'asr_tentative_pause':
        // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏ "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–º–æ–ª—á–∞–ª"
        console.log('üîá User paused (tentative)');
        break;

      case 'metric':
        // –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç –±—ç–∫–µ–Ω–¥–∞ (–ª–∞—Ç–µ–Ω—Å–∏ –∏ —Ç.–¥.)
        if (message.metrics) {
          console.log('üìä Metrics:', message.metrics);
        }
        break;

      default:
        console.log('Unknown message type:', message.type);
    }
  }, [onTranscript, onLLMResponse]);

   /**
    * Play audio chunk using AudioContext for better reliability
    */
   const playNextAudio = useCallback(async () => {
     // –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π AudioContext –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
     if (!playbackAudioContextRef.current) {
       try {
         playbackAudioContextRef.current = new AudioContext({ sampleRate: 16000 });
         console.log('‚úÖ Created playback AudioContext');
       } catch (error) {
         console.error('‚ùå Failed to create playback AudioContext:', error);
         isPlayingRef.current = false;
         return;
       }
     }

     const playbackCtx = playbackAudioContextRef.current;

     if (audioQueueRef.current.length === 0) {
       isPlayingRef.current = false;
       return;
     }
 
     isPlayingRef.current = true;
     const wavBytes = audioQueueRef.current.shift()!;
     
     try {
       // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ AudioContext
       console.log(`üéµ AudioContext state before: ${playbackCtx.state}`);
       
       if (playbackCtx.state === 'suspended') {
         await playbackCtx.resume();
         console.log(`‚úÖ Resumed playback AudioContext, new state: ${playbackCtx.state}`);
       }

       if (playbackCtx.state === 'closed') {
         console.error('‚ùå Playback AudioContext is closed, recreating...');
         playbackAudioContextRef.current = new AudioContext({ sampleRate: 16000 });
         return playNextAudio();
       }

       // –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ 'running'
       if (playbackCtx.state !== 'running') {
         console.warn(`‚ö†Ô∏è AudioContext not running (state: ${playbackCtx.state}), attempting to resume...`);
         await playbackCtx.resume();
       }

       // –î–µ–∫–æ–¥–∏—Ä—É–µ–º WAV –¥–∞–Ω–Ω—ã–µ
       const audioData = wavBytes.buffer.slice(wavBytes.byteOffset, wavBytes.byteOffset + wavBytes.byteLength);
       console.log(`üéµ Decoding audio: ${audioData.byteLength} bytes`);
       const audioBuffer = await playbackCtx.decodeAudioData(audioData as ArrayBuffer);
       console.log(`‚úÖ Audio decoded: ${audioBuffer.duration.toFixed(2)}s, ${audioBuffer.numberOfChannels} channels, ${audioBuffer.sampleRate}Hz`);
       
       const source = playbackCtx.createBufferSource();
       source.buffer = audioBuffer;
       
       // –°–æ–∑–¥–∞–µ–º GainNode –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏
       const gainNode = playbackCtx.createGain();
       gainNode.gain.value = 1.0; // –ü–æ–ª–Ω–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å
       source.connect(gainNode);
       gainNode.connect(playbackCtx.destination);
       
       source.onended = () => {
         console.log('‚úÖ Audio chunk playback ended');
         playNextAudio();
       };
 
       source.start(0);
       console.log(`üîä Audio playback started! State: ${playbackCtx.state}, Duration: ${audioBuffer.duration.toFixed(2)}s`);
     } catch (error) {
       console.error('‚ùå Audio playback error:', error);
       playNextAudio();
     }
   }, []);

  /**
   * Play audio chunk
   */
  const playAudioChunk = useCallback((wavBytes: Uint8Array) => {
    audioQueueRef.current.push(wavBytes);
    if (!isPlayingRef.current) {
      playNextAudio();
    }
  }, [playNextAudio]);

  /**
   * Handle binary audio data from server (TTS)
   */
  const handleBinaryAudio = useCallback((data: ArrayBuffer) => {
    try {
      const buffer = new Uint8Array(data);
      let wavBytes: Uint8Array;
 
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ AUD0 (–ø—Ä–æ—Ç–æ–∫–æ–ª v1)
      const isV1 = buffer.length > 14 && 
                   buffer[0] === 65 && buffer[1] === 85 && 
                   buffer[2] === 68 && buffer[3] === 48; // "AUD0"

      if (isV1) {
        const view = new DataView(buffer.buffer, buffer.byteOffset, buffer.byteLength);
        const payloadLen = view.getUint32(10, true);
        wavBytes = buffer.slice(14, 14 + payloadLen);
      } else {
        // –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ AUD0, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ —á–∏—Å—Ç—ã–π WAV (–ø—Ä–æ—Ç–æ–∫–æ–ª v2)
        wavBytes = buffer;
      }
 
      if (wavBytes.length > 0) {
        ttsChunkCountRef.current++;
        console.log(`üì• Processed audio chunk #${ttsChunkCountRef.current}: ${wavBytes.length} bytes (protocol: ${isV1 ? 'v1' : 'v2'}, TTS active: ${isTTSActiveRef.current})`);
      }

      if (audioEnabled && wavBytes.length > 0) {
        setCallState('speaking');
        console.log(`üéµ Adding to playback queue (queue length: ${audioQueueRef.current.length}, total chunks: ${ttsChunkCountRef.current})`);
        playAudioChunk(wavBytes);
      } else if (!audioEnabled) {
        console.log('üîá Audio disabled, skipping playback');
      } else {
        console.warn('‚ö†Ô∏è Empty audio chunk received');
      }
    } catch (error) {
      console.error('‚ùå Error parsing binary audio:', error);
    }
  }, [audioEnabled, playAudioChunk]);

  /**
   * Start voice call
   */
  const startCall = useCallback(async () => {
    if (isStartingRef.current || callState !== 'idle') {
      console.log('‚ö†Ô∏è Call already starting or active, skipping startCall');
      return;
    }

    isStartingRef.current = true;
    try {
      // Connect WebSocket if not connected
      if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
        connectWebSocket();
        
        // Wait for WebSocket connection (max 5 seconds)
        let attempts = 0;
        while (attempts < 50 && (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN)) {
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }
        
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          throw new Error('WebSocket connection timeout');
        }
        
        // Wait for backend ready (max 5 seconds)
        attempts = 0;
        while (attempts < 50 && !backendReadyRef.current) {
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }
        
        if (!backendReadyRef.current) {
          throw new Error('Backend not ready (timeout)');
        }
      } else {
        // WebSocket connected, but check if backend is ready
        if (!backendReadyRef.current) {
          // Wait for backend ready (max 3 seconds)
          let attempts = 0;
          while (attempts < 30 && !backendReadyRef.current) {
            await new Promise(resolve => setTimeout(resolve, 100));
            attempts++;
          }
          
          if (!backendReadyRef.current) {
            throw new Error('Backend not ready');
          }
        }
      }

      // Get microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        }
      });

      streamRef.current = stream;

      // Create AudioContext
      const audioContext = new AudioContext({ sampleRate: 16000 });
      audioContextRef.current = audioContext;

      const source = audioContext.createMediaStreamSource(stream);

      // Load AudioWorklet
      await audioContext.audioWorklet.addModule('/audioWorklet.js');
      const workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
      workletNodeRef.current = workletNode;

      // Initialize worklet with actual sample rate
      workletNode.port.postMessage({
        type: 'init',
        sampleRate: audioContext.sampleRate
      });

      // Handle PCM data from worklet
      workletNode.port.onmessage = (event) => {
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          console.warn('‚ö†Ô∏è WebSocket not ready, skipping PCM');
          return;
        }
        if (!backendReadyRef.current) {
          // –ù–µ –ª–æ–≥–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
          if (Math.random() < 0.01) {
            console.warn('‚ö†Ô∏è Backend not ready yet, skipping PCM');
          }
          return;
        }
        if (isMutedRef.current) {
          // Skip sending if muted
          return;
        }

        const { pcm } = event.data;
        if (pcm && pcm.byteLength > 0) {
          // –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞–∫–µ—Ç–æ–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –∫–æ–Ω—Å–æ–ª—å
          if (Math.random() < 0.01) { // ~1% –ø–∞–∫–µ—Ç–æ–≤
            console.log(`üì§ Sending PCM: ${pcm.byteLength} bytes`);
          }
          wsRef.current.send(pcm);
        } else {
          console.warn('‚ö†Ô∏è Empty PCM data');
        }
      };

      source.connect(workletNode);
      
      // –°–æ–∑–¥–∞–µ–º –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º AudioContext –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ
      if (!playbackAudioContextRef.current) {
        try {
          playbackAudioContextRef.current = new AudioContext({ sampleRate: 16000 });
          // –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∑–≤–æ–Ω–∫–∞ (user interaction —É–∂–µ –µ—Å—Ç—å)
          if (playbackAudioContextRef.current.state === 'suspended') {
            await playbackAudioContextRef.current.resume();
          }
          console.log(`‚úÖ Playback AudioContext created and activated: ${playbackAudioContextRef.current.state}`);
        } catch (error) {
          console.error('‚ö†Ô∏è Failed to create playback AudioContext:', error);
        }
      }
      
      setCallState('active');
      console.log('‚úÖ Voice call started');
      console.log('üé§ Microphone stream active, worklet connected');
      console.log(`üìä Stream settings: ${stream.getAudioTracks()[0]?.getSettings()?.sampleRate}Hz`);
    } catch (error) {
      console.error('Failed to start call:', error);
      setError('–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–π –∑–≤–æ–Ω–æ–∫. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É.');
    } finally {
      isStartingRef.current = false;
    }
  }, [connectWebSocket]); // Removed isMuted from deps

  /**
   * Stop voice call
   */
  const stopCall = useCallback(() => {
    try {
      // Send EOF to server
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ eof: 1 }));
      }

      // Stop microphone
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }

      // Disconnect worklet
      if (workletNodeRef.current) {
        workletNodeRef.current.disconnect();
        workletNodeRef.current = null;
      }

      // Close AudioContext for recording
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }

      // Close AudioContext for playback
      if (playbackAudioContextRef.current) {
        playbackAudioContextRef.current.close();
        playbackAudioContextRef.current = null;
      }

      // Clear audio queue
      audioQueueRef.current = [];
      isPlayingRef.current = false;

      setCallState('idle');
      setPartialTranscript('');
      console.log('‚úÖ Voice call stopped');
    } catch (error) {
      console.error('Error stopping call:', error);
    }
  }, []);

  /**
   * Cleanup on unmount
   */
  useEffect(() => {
    if (autoStart) {
      startCall();
    }
    return () => {
      stopCall();
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, [autoStart]); // eslint-disable-line react-hooks/exhaustive-deps

  return (
    <Card className={cn('p-4 space-y-4 bg-background/95 backdrop-blur border-primary/20 shadow-lg animate-in fade-in zoom-in duration-300', className)}>
      {/* Connection Status & Call State */}
      <div className="flex items-center justify-between">
        <div className="flex items-center gap-2">
          <div className={cn(
            'w-2 h-2 rounded-full',
            connectionState === 'connected' && 'bg-green-500 animate-pulse',
            connectionState === 'connecting' && 'bg-yellow-500',
            connectionState === 'disconnected' && 'bg-gray-400',
            connectionState === 'error' && 'bg-red-500'
          )} />
          <span className="text-xs font-medium text-muted-foreground">
            {connectionState === 'connected' && 'AI –Ω–∞ —Å–≤—è–∑–∏'}
            {connectionState === 'connecting' && '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...'}
            {connectionState === 'disconnected' && '–û–∂–∏–¥–∞–Ω–∏–µ'}
            {connectionState === 'error' && '–û—à–∏–±–∫–∞'}
          </span>
        </div>

        {callState !== 'idle' && (
          <div className="text-xs font-medium px-2 py-1 rounded-full bg-primary/10 text-primary animate-pulse">
            {callState === 'active' && 'üé§ –ì–æ–≤–æ—Ä–∏—Ç–µ'}
            {callState === 'listening' && 'üëÇ –°–ª—É—à–∞—é...'}
            {callState === 'speaking' && 'üó£Ô∏è AI –æ—Ç–≤–µ—á–∞–µ—Ç'}
          </div>
        )}
      </div>

      {/* Main Controls - simplified */}
      <div className="flex items-center justify-center gap-3">
        {callState === 'idle' ? (
          <Button
            size="lg"
            onClick={startCall}
            disabled={connectionState === 'connecting'}
            className="rounded-full w-full py-6 gap-3 text-lg font-semibold shadow-md hover:shadow-lg transition-all"
          >
            <Phone className="w-6 h-6" />
            –ù–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫
          </Button>
        ) : (
          <div className="flex items-center gap-3 w-full">
            <Button
              size="icon"
              variant="destructive"
              onClick={stopCall}
              className="rounded-full h-12 w-12 shrink-0 shadow-md hover:shadow-lg transition-all"
              title="–ó–∞–≤–µ—Ä—à–∏—Ç—å –∑–≤–æ–Ω–æ–∫"
            >
              <PhoneOff className="w-6 h-6" />
            </Button>

            <div className="flex-1 h-12 bg-secondary/50 rounded-full flex items-center px-4 overflow-hidden">
              <div className="flex-1 overflow-hidden">
                {callState === 'speaking' ? (
                  <div className="flex gap-1 items-center justify-center">
                    <div className="w-1 h-4 bg-primary/50 rounded-full animate-wave" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full animate-wave [animation-delay:0.1s]" />
                    <div className="w-1 h-8 bg-primary/50 rounded-full animate-wave [animation-delay:0.2s]" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full animate-wave [animation-delay:0.3s]" />
                    <div className="w-1 h-4 bg-primary/50 rounded-full animate-wave [animation-delay:0.4s]" />
                  </div>
                ) : partialTranscript ? (
                  <p className="text-sm text-foreground truncate italic">
                    {partialTranscript}
                  </p>
                ) : (
                  <div className="flex gap-1 items-center justify-center opacity-30">
                    <div className="w-1 h-4 bg-primary/50 rounded-full" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full" />
                    <div className="w-1 h-8 bg-primary/50 rounded-full" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full" />
                    <div className="w-1 h-4 bg-primary/50 rounded-full" />
                  </div>
                )}
              </div>
            </div>

            <Button
              size="icon"
              variant={isMuted ? 'secondary' : 'outline'}
              onClick={() => setIsMuted(!isMuted)}
              className={cn("rounded-full h-10 w-10 shrink-0 transition-all", isMuted && "bg-red-100 text-red-600 border-red-200")}
              title={isMuted ? '–í–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω' : '–í—ã–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω'}
            >
              {isMuted ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
            </Button>
            
            <Button
              size="icon"
              variant={audioEnabled ? 'outline' : 'secondary'}
              onClick={() => setAudioEnabled(!audioEnabled)}
              className="rounded-full h-10 w-10 shrink-0 transition-all"
              title={audioEnabled ? '–í—ã–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫' : '–í–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫'}
            >
              {audioEnabled ? <Volume2 className="w-5 h-5" /> : <VolumeX className="w-5 h-5" />}
            </Button>
          </div>
        )}
      </div>

      {error && (
        <div className="text-xs text-red-500 text-center bg-red-50 p-2 rounded border border-red-100">
          {error}
        </div>
      )}
    </Card>
  );
};

export default VoiceCall;
