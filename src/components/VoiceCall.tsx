import React, { useEffect, useRef, useState, useCallback } from 'react';
import { Mic, MicOff, Phone, PhoneOff, Volume2, VolumeX } from 'lucide-react';
import { Button } from './ui/button';
import { Card } from './ui/card';
import { cn } from '@/lib/utils';

interface VoiceCallProps {
  wsUrl?: string;
  onTranscript?: (text: string, isFinal: boolean) => void;
  onLLMResponse?: (delta: string, isStart: boolean, isEnd: boolean) => void;
  className?: string;
  autoStart?: boolean;
}

type ConnectionState = 'disconnected' | 'connecting' | 'connected' | 'error';
type CallState = 'idle' | 'active' | 'speaking' | 'listening';

export const VoiceCall: React.FC<VoiceCallProps> = ({
  wsUrl = window.location.protocol === 'https:' 
    ? `wss://${window.location.hostname}/ws-voice/`
    : `ws://${window.location.hostname}:2700`,
  onTranscript,
  onLLMResponse,
  className,
  autoStart = false
}) => {
  // Connection state
  const [connectionState, setConnectionState] = useState<ConnectionState>('disconnected');
  const [callState, setCallState] = useState<CallState>('idle');
  const [isMuted, setIsMuted] = useState(false);
  const [audioEnabled, setAudioEnabled] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // Transcript state
  const [partialTranscript, setPartialTranscript] = useState('');
  const [finalTranscript, setFinalTranscript] = useState('');
  const [llmResponse, setLlmResponse] = useState('');
  const [protocolVersion, setProtocolVersion] = useState(1);
  const backendReadyRef = useRef(false);

  // Refs
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const playbackAudioContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioQueueRef = useRef<Uint8Array[]>([]);
  const isPlayingRef = useRef(false);
  const isStartingRef = useRef(false);
  const isMutedRef = useRef(false);
  const isLLMRespondingRef = useRef(false);
  const ttsChunkCountRef = useRef(0);
  const isTTSActiveRef = useRef(false);
  const [audioLevels, setAudioLevels] = useState<number[]>([0.3, 0.5, 0.7, 0.5, 0.3]);
  const [isMediaDevicesSupported, setIsMediaDevicesSupported] = useState<boolean | null>(null);
  const audioLevelsIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const stopAnimationTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const analyserNodeRef = useRef<AnalyserNode | null>(null);
  const isAudioPlayingRef = useRef(false);

  // Check MediaDevices API availability on mount
  useEffect(() => {
    const checkMediaDevices = () => {
      const isSupported = !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
      setIsMediaDevicesSupported(isSupported);
      if (!isSupported) {
        const isHttp = window.location.protocol === 'http:';
        const hostname = window.location.hostname;
        if (isHttp && hostname !== 'localhost' && !hostname.startsWith('127.0.0.1')) {
          setError(`‚ö†Ô∏è –ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ HTTP.\n\n–î–ª—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è HTTPS.\n–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: https://chat.tartihome.online\n\n–ò–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ chrome://flags –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.`);
        } else {
          setError('–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —ç—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ.');
        }
      }
    };
    checkMediaDevices();
  }, []);

  // Sync ref with state
  useEffect(() => {
    isMutedRef.current = isMuted;
  }, [isMuted]);

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –∑–≤—É–∫–æ–≤–æ–π –≤–æ–ª–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∞—É–¥–∏–æ
  const startAudioWaveAnimation = useCallback(() => {
    // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏ —Ç–∞–π–º–∞—É—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    if (audioLevelsIntervalRef.current) {
      clearInterval(audioLevelsIntervalRef.current);
    }
    if (stopAnimationTimeoutRef.current) {
      clearTimeout(stopAnimationTimeoutRef.current);
      stopAnimationTimeoutRef.current = null;
    }

    // –°–æ–∑–¥–∞–µ–º –∞–Ω–∏–º–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ
    audioLevelsIntervalRef.current = setInterval(() => {
      const analyser = analyserNodeRef.current;
      
      if (analyser && isAudioPlayingRef.current) {
        // –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyser.getByteFrequencyData(dataArray);
        
        // –†–∞–∑–±–∏–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–∞ 5 –ø–æ–ª–æ—Å –¥–ª—è 5 –±–∞—Ä–æ–≤
        const bands = 5;
        const bandSize = Math.floor(bufferLength / bands);
        const levels: number[] = [];
        
        for (let i = 0; i < bands; i++) {
          let sum = 0;
          const start = i * bandSize;
          const end = start + bandSize;
          
          for (let j = start; j < end; j++) {
            sum += dataArray[j];
          }
          
          // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1
          const average = sum / bandSize;
          const normalized = average / 255;
          
          // –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∏—è—Ç–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
          const scaled = Math.pow(normalized, 0.5); // –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –¥–ª—è –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
          
          // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤–∏–¥–∏–º–æ—Å—Ç–∏
          levels.push(Math.max(0.2, Math.min(1.0, scaled * 2)));
        }
        
        setAudioLevels(levels);
      } else {
        // –ï—Å–ª–∏ –∞—É–¥–∏–æ –Ω–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        setAudioLevels([0.3, 0.5, 0.7, 0.5, 0.3]);
      }
    }, 50); // –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 50–º—Å –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
  }, []);

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–Ω–∏–º–∞—Ü–∏–∏
  const stopAudioWaveAnimation = useCallback((delay = 1500) => {
    // –ï—Å–ª–∏ —É–∂–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞, –Ω–µ –¥–µ–ª–∞–µ–º –Ω–∏—á–µ–≥–æ
    if (stopAnimationTimeoutRef.current) return;

    stopAnimationTimeoutRef.current = setTimeout(() => {
      if (audioLevelsIntervalRef.current) {
        clearInterval(audioLevelsIntervalRef.current);
        audioLevelsIntervalRef.current = null;
      }
      setAudioLevels([0.3, 0.5, 0.7, 0.5, 0.3]);
      stopAnimationTimeoutRef.current = null;
    }, delay);
  }, []);

  // –û—á–∏—Å—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
  useEffect(() => {
    return () => {
      if (audioLevelsIntervalRef.current) {
        clearInterval(audioLevelsIntervalRef.current);
      }
      if (stopAnimationTimeoutRef.current) {
        clearTimeout(stopAnimationTimeoutRef.current);
      }
    };
  }, []);

  /**
   * Initialize WebSocket connection
   */
  const connectWebSocket = useCallback(() => {
    if (wsRef.current?.readyState === WebSocket.OPEN) return;

    setConnectionState('connecting');
    
    try {
      const ws = new WebSocket(wsUrl);
      ws.binaryType = 'arraybuffer';

      ws.onopen = () => {
        console.log('üîå WebSocket connected');
        setConnectionState('connected');
        
        // Send config
        ws.send(JSON.stringify({
          config: {
            sample_rate: 16000,
            words: false,
            protocol_version: 2
          }
        }));
      };

      ws.onmessage = async (event) => {
        if (event.data instanceof ArrayBuffer) {
          console.log(`üì¶ Received binary data: ${event.data.byteLength} bytes (TTS active: ${isTTSActiveRef.current}, chunk #${ttsChunkCountRef.current + 1})`);
          // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
          handleBinaryAudio(event.data);
          return;
        }

        try {
          const message = JSON.parse(event.data);
          // –ë—ç–∫–µ–Ω–¥ –º–æ–∂–µ—Ç –ø—Ä–∏—Å—ã–ª–∞—Ç—å —Ç–∏–ø –≤ –ø–æ–ª–µ 'type' –∏–ª–∏ 'event'
          const msgType = message.type || message.event;
          console.log(`üì® Received message: ${msgType}`, message);
          handleWebSocketMessage({ ...message, type: msgType });
        } catch (error) {
          console.error('Failed to parse message:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        setConnectionState('error');
      };

      ws.onclose = () => {
        console.log('üîå WebSocket disconnected');
        setConnectionState('disconnected');
        backendReadyRef.current = false;
        wsRef.current = null;
      };

      wsRef.current = ws;
    } catch (error) {
      console.error('Failed to connect WebSocket:', error);
      setConnectionState('error');
    }
  }, [wsUrl]);

  /**
   * Handle WebSocket messages
   */
  const handleWebSocketMessage = useCallback((message: any) => {
    switch (message.type) {
      case 'partial':
        setPartialTranscript(message.partial || '');
        onTranscript?.(message.partial || '', false);
        setCallState('listening');
        break;

      case 'final':
        if (message.text?.trim()) {
          setFinalTranscript(prev => prev + ' ' + message.text);
          onTranscript?.(message.text, true);
          setPartialTranscript('');
        }
        break;

      case 'nlu_start':
      case 'llm_start':
        if (!isLLMRespondingRef.current) {
          isLLMRespondingRef.current = true;
          setLlmResponse('');
          setCallState('speaking');
          onLLMResponse?.('', true, false); // isStart = true
          // –ê–Ω–∏–º–∞—Ü–∏—è –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –∞—É–¥–∏–æ
        }
        break;

      case 'llm_delta':
        setLlmResponse(prev => prev + (message.delta || ''));
        onLLMResponse?.(message.delta || '', false, false); // delta chunk
        break;

      case 'nlu_end':
      case 'llm_end':
        isLLMRespondingRef.current = false;
        setCallState('active');
        onLLMResponse?.('', false, true); // isEnd = true
        // –ê–Ω–∏–º–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
        break;

      case 'llm_error':
        isLLMRespondingRef.current = false;
        console.error('LLM error:', message.error);
        setCallState('active');
        // –£–≤–µ–¥–æ–º–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –æ–± –æ—à–∏–±–∫–µ, —á—Ç–æ–±—ã —É–¥–∞–ª–∏—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        onLLMResponse?.('', false, true); // isEnd = true, —á—Ç–æ–±—ã –æ—á–∏—Å—Ç–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        break;

      case 'abort':
        console.log('üõë Abort received:', message.reason);
        audioQueueRef.current = [];
        isPlayingRef.current = false;
        setCallState('active');
        break;

        case 'tts_start':
          console.log('üîä TTS started');
          isTTSActiveRef.current = true;
          ttsChunkCountRef.current = 0;
          setCallState('speaking');
          // –ê–Ω–∏–º–∞—Ü–∏—è –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –∞—É–¥–∏–æ
          break;

      case 'tts_end':
        console.log(`üîä TTS ended (received ${ttsChunkCountRef.current} audio chunks)`);
        isTTSActiveRef.current = false;
        // –ê–Ω–∏–º–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
        // –ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ idle, —á—Ç–æ–±—ã –∑–≤–æ–Ω–æ–∫ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–ª—Å—è
        if (protocolVersion < 2) {
          setCallState('active');
        }
        break;

      case 'tts_error':
        console.error('TTS error:', message.error);
        break;

      case 'tts_audio':
        // –≠—Ç–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –±–∏–Ω–∞—Ä–Ω—ã–º –∞—É–¥–∏–æ-—á–∞–Ω–∫–æ–º
        // –°–ª–µ–¥—É—é—â–∏–π –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ—Ä–µ–π–º –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞—É–¥–∏–æ
        console.log(`üéµ TTS audio metadata: utterance_id=${message.utterance_id}, mime=${message.mime}`);
        // –ù–µ –Ω—É–∂–Ω–æ –Ω–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å, –ø—Ä–æ—Å—Ç–æ –∂–¥–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        break;

      case 'ready':
        console.log('‚úÖ Backend ready', message);
        if (message.protocol_version) {
          setProtocolVersion(message.protocol_version);
        }
        backendReadyRef.current = true;
        console.log('‚úÖ Ready received, can start sending PCM');
        break;

      case 'pong':
        // Keep-alive response
        break;

      case 'asr_tentative_pause':
        // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏ "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–º–æ–ª—á–∞–ª"
        console.log('üîá User paused (tentative)');
        break;

      case 'metric':
        // –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç –±—ç–∫–µ–Ω–¥–∞ (–ª–∞—Ç–µ–Ω—Å–∏ –∏ —Ç.–¥.)
        if (message.metrics) {
          console.log('üìä Metrics:', message.metrics);
        }
        break;

      default:
        console.log('Unknown message type:', message.type);
    }
  }, [onTranscript, onLLMResponse]);

  /**
   * Play audio chunk using AudioContext for better reliability
   */
  const playNextAudio = useCallback(async () => {
    // –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π AudioContext –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if (!playbackAudioContextRef.current) {
      try {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π sample rate —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ iPhone
        const nativeSampleRate = new AudioContext().sampleRate;
        playbackAudioContextRef.current = new AudioContext({ sampleRate: nativeSampleRate });
        console.log(`‚úÖ Created playback AudioContext with native sample rate: ${nativeSampleRate}Hz`);
      } catch (error) {
        console.error('‚ùå Failed to create playback AudioContext:', error);
        isPlayingRef.current = false;
        return;
      }
    }

     const playbackCtx = playbackAudioContextRef.current;

     if (audioQueueRef.current.length === 0) {
       isPlayingRef.current = false;
      isAudioPlayingRef.current = false;
      stopAudioWaveAnimation(500); // –ü–ª–∞–≤–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–Ω–∏–º–∞—Ü–∏–∏
       return;
     }
 
     isPlayingRef.current = true;
     const wavBytes = audioQueueRef.current.shift()!;
     
     try {
       // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ AudioContext
       console.log(`üéµ AudioContext state before: ${playbackCtx.state}`);
       
       if (playbackCtx.state === 'suspended') {
         await playbackCtx.resume();
         console.log(`‚úÖ Resumed playback AudioContext, new state: ${playbackCtx.state}`);
       }

      if (playbackCtx.state === 'closed') {
        console.error('‚ùå Playback AudioContext is closed, recreating...');
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π sample rate —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ iPhone
        const nativeSampleRate = new AudioContext().sampleRate;
        playbackAudioContextRef.current = new AudioContext({ sampleRate: nativeSampleRate });
        console.log(`‚úÖ Recreated playback AudioContext with native sample rate: ${nativeSampleRate}Hz`);
        return playNextAudio();
      }

       // –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ 'running'
       if (playbackCtx.state !== 'running') {
         console.warn(`‚ö†Ô∏è AudioContext not running (state: ${playbackCtx.state}), attempting to resume...`);
         await playbackCtx.resume();
       }

       // –î–µ–∫–æ–¥–∏—Ä—É–µ–º WAV –¥–∞–Ω–Ω—ã–µ
       const audioData = wavBytes.buffer.slice(wavBytes.byteOffset, wavBytes.byteOffset + wavBytes.byteLength);
       console.log(`üéµ Decoding audio: ${audioData.byteLength} bytes`);
       const audioBuffer = await playbackCtx.decodeAudioData(audioData as ArrayBuffer);
       console.log(`‚úÖ Audio decoded: ${audioBuffer.duration.toFixed(2)}s, ${audioBuffer.numberOfChannels} channels, ${audioBuffer.sampleRate}Hz`);
       
       const source = playbackCtx.createBufferSource();
       source.buffer = audioBuffer;
      
      // –°–æ–∑–¥–∞–µ–º AnalyserNode –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
      // –í–∞–∂–Ω–æ: —Å–æ–∑–¥–∞–µ–º –≤ —Ç–æ–º –∂–µ AudioContext, —á—Ç–æ –∏ source
      // –ï—Å–ª–∏ AnalyserNode –±—ã–ª —Å–æ–∑–¥–∞–Ω –≤ –¥—Ä—É–≥–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –µ–≥–æ
      if (!analyserNodeRef.current || analyserNodeRef.current.context !== playbackCtx) {
        analyserNodeRef.current = playbackCtx.createAnalyser();
        analyserNodeRef.current.fftSize = 256; // –†–∞–∑–º–µ—Ä FFT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analyserNodeRef.current.smoothingTimeConstant = 0.8; // –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
      }
       
       // –°–æ–∑–¥–∞–µ–º GainNode –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏
       const gainNode = playbackCtx.createGain();
       gainNode.gain.value = 1.0; // –ü–æ–ª–Ω–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å
      
      // –ü–æ–¥–∫–ª—é—á–∞–µ–º —Ü–µ–ø–æ—á–∫—É: source -> gain -> analyser -> destination
       source.connect(gainNode);
      gainNode.connect(analyserNodeRef.current);
      analyserNodeRef.current.connect(playbackCtx.destination);
      
      // –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∏–º–∞—Ü–∏—é –ø—Ä–∏ –Ω–∞—á–∞–ª–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
      isAudioPlayingRef.current = true;
      startAudioWaveAnimation();
       
       source.onended = () => {
         console.log('‚úÖ Audio chunk playback ended');
        // –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–Ω–∏–º–∞—Ü–∏—é
        if (audioQueueRef.current.length === 0) {
          isAudioPlayingRef.current = false;
          stopAudioWaveAnimation(500); // –ü–ª–∞–≤–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ 500–º—Å
        }
         playNextAudio();
       };
 
       source.start(0);
       console.log(`üîä Audio playback started! State: ${playbackCtx.state}, Duration: ${audioBuffer.duration.toFixed(2)}s`);
     } catch (error) {
       console.error('‚ùå Audio playback error:', error);
       playNextAudio();
     }
   }, []);

  /**
   * Play audio chunk
   */
  const playAudioChunk = useCallback((wavBytes: Uint8Array) => {
    audioQueueRef.current.push(wavBytes);
    if (!isPlayingRef.current) {
      playNextAudio();
    }
  }, [playNextAudio]);

  /**
   * Handle binary audio data from server (TTS)
   */
  const handleBinaryAudio = useCallback((data: ArrayBuffer) => {
    try {
      const buffer = new Uint8Array(data);
      let wavBytes: Uint8Array;
 
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ AUD0 (–ø—Ä–æ—Ç–æ–∫–æ–ª v1)
      const isV1 = buffer.length > 14 && 
                   buffer[0] === 65 && buffer[1] === 85 && 
                   buffer[2] === 68 && buffer[3] === 48; // "AUD0"

      if (isV1) {
        const view = new DataView(buffer.buffer, buffer.byteOffset, buffer.byteLength);
        const payloadLen = view.getUint32(10, true);
        wavBytes = buffer.slice(14, 14 + payloadLen);
      } else {
        // –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ AUD0, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ —á–∏—Å—Ç—ã–π WAV (–ø—Ä–æ—Ç–æ–∫–æ–ª v2)
        wavBytes = buffer;
      }
 
      if (wavBytes.length > 0) {
        ttsChunkCountRef.current++;
        console.log(`üì• Processed audio chunk #${ttsChunkCountRef.current}: ${wavBytes.length} bytes (protocol: ${isV1 ? 'v1' : 'v2'}, TTS active: ${isTTSActiveRef.current})`);
      }

      if (audioEnabled && wavBytes.length > 0) {
        setCallState('speaking');
        console.log(`üéµ Adding to playback queue (queue length: ${audioQueueRef.current.length}, total chunks: ${ttsChunkCountRef.current})`);
        playAudioChunk(wavBytes);
      } else if (!audioEnabled) {
        console.log('üîá Audio disabled, skipping playback');
      } else {
        console.warn('‚ö†Ô∏è Empty audio chunk received');
      }
    } catch (error) {
      console.error('‚ùå Error parsing binary audio:', error);
    }
  }, [audioEnabled, playAudioChunk]);

  /**
   * Start voice call
   */
  const startCall = useCallback(async () => {
    if (isStartingRef.current || callState !== 'idle') {
      console.log('‚ö†Ô∏è Call already starting or active, skipping startCall');
      return;
    }

    isStartingRef.current = true;
    try {
      // Connect WebSocket if not connected
      if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
        connectWebSocket();
        
        // Wait for WebSocket connection (max 5 seconds)
        let attempts = 0;
        while (attempts < 50 && (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN)) {
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }
        
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          throw new Error('WebSocket connection timeout');
        }
        
        // Wait for backend ready (max 5 seconds)
        attempts = 0;
        while (attempts < 50 && !backendReadyRef.current) {
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }
        
        if (!backendReadyRef.current) {
          throw new Error('Backend not ready (timeout)');
        }
      } else {
        // WebSocket connected, but check if backend is ready
        if (!backendReadyRef.current) {
          // Wait for backend ready (max 3 seconds)
          let attempts = 0;
          while (attempts < 30 && !backendReadyRef.current) {
            await new Promise(resolve => setTimeout(resolve, 100));
            attempts++;
          }
          
          if (!backendReadyRef.current) {
            throw new Error('Backend not ready');
          }
        }
      }

      // Check if mediaDevices API is available
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('MediaDevices API is not available. Please use HTTPS or a browser that supports microphone access.');
      }

      // Get microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        }
      });

      streamRef.current = stream;

      // Create AudioContext with native sample rate (browsers may not support 16kHz)
      // We'll resample to 16kHz in the AudioWorklet
      const nativeSampleRate = new AudioContext().sampleRate;
      const audioContext = new AudioContext({ sampleRate: nativeSampleRate });
      audioContextRef.current = audioContext;

      const source = audioContext.createMediaStreamSource(stream);

      // Load AudioWorklet with error handling
      try {
        console.log('üì¶ Loading AudioWorklet module...');
        // Use unique URL to bypass cache
        await audioContext.audioWorklet.addModule(`/audioWorklet.js?v=${Date.now()}`);
        console.log('‚úÖ AudioWorklet module loaded successfully');
        
        // Ensure AudioContext is running before creating nodes
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
          console.log('‚úÖ AudioContext resumed');
        }
        
        // Small delay to ensure processor is registered
        await new Promise(resolve => setTimeout(resolve, 100));
        
        console.log('üîß Creating AudioWorkletNode...');
        const workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        workletNodeRef.current = workletNode;
        console.log('‚úÖ AudioWorkletNode created successfully');
      } catch (workletError: any) {
        console.error('‚ùå AudioWorklet error:', workletError);
        throw new Error(`Failed to load AudioWorklet: ${workletError.message}. Make sure audioWorklet.js is accessible at /audioWorklet.js`);
      }

      // Initialize worklet with actual sample rate
      workletNode.port.postMessage({
        type: 'init',
        sampleRate: audioContext.sampleRate
      });

      // Handle PCM data from worklet
      workletNode.port.onmessage = (event) => {
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          console.warn('‚ö†Ô∏è WebSocket not ready, skipping PCM');
          return;
        }
        if (!backendReadyRef.current) {
          // –ù–µ –ª–æ–≥–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
          if (Math.random() < 0.01) {
            console.warn('‚ö†Ô∏è Backend not ready yet, skipping PCM');
          }
          return;
        }
        if (isMutedRef.current) {
          // Skip sending if muted
          return;
        }

        const { pcm } = event.data;
        if (pcm && pcm.byteLength > 0) {
          // –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞–∫–µ—Ç–æ–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –∫–æ–Ω—Å–æ–ª—å
          if (Math.random() < 0.01) { // ~1% –ø–∞–∫–µ—Ç–æ–≤
            console.log(`üì§ Sending PCM: ${pcm.byteLength} bytes`);
          }
          wsRef.current.send(pcm);
        } else {
          console.warn('‚ö†Ô∏è Empty PCM data');
        }
      };

      source.connect(workletNode);
      
      // –°–æ–∑–¥–∞–µ–º –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º AudioContext –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ
      if (!playbackAudioContextRef.current) {
        try {
          // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π sample rate —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ iPhone
          const nativeSampleRate = new AudioContext().sampleRate;
          playbackAudioContextRef.current = new AudioContext({ sampleRate: nativeSampleRate });
          // –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∑–≤–æ–Ω–∫–∞ (user interaction —É–∂–µ –µ—Å—Ç—å)
          if (playbackAudioContextRef.current.state === 'suspended') {
            await playbackAudioContextRef.current.resume();
          }
          console.log(`‚úÖ Playback AudioContext created and activated: ${playbackAudioContextRef.current.state}, sample rate: ${nativeSampleRate}Hz`);
        } catch (error) {
          console.error('‚ö†Ô∏è Failed to create playback AudioContext:', error);
        }
      }
      
      setCallState('active');
      console.log('‚úÖ Voice call started');
      console.log('üé§ Microphone stream active, worklet connected');
      console.log(`üìä Stream settings: ${stream.getAudioTracks()[0]?.getSettings()?.sampleRate}Hz`);
    } catch (error: any) {
      console.error('Failed to start call:', error);
      let errorMessage = '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–π –∑–≤–æ–Ω–æ–∫.';
      
      if (error?.message?.includes('MediaDevices API')) {
        const isHttp = window.location.protocol === 'http:';
        const hostname = window.location.hostname;
        errorMessage = isHttp 
          ? `‚ö†Ô∏è –î–ª—è —Ä–∞–±–æ—Ç—ã –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è HTTPS.\n\n–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–æ–º–µ–Ω —Å SSL:\nhttps://chat.tartihome.online\n\n–ò–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ HTTPS –¥–ª—è ${hostname}`
          : '–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MediaDevices API.';
      } else if (error?.name === 'NotAllowedError' || error?.name === 'PermissionDeniedError') {
        errorMessage = '–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.';
      } else if (error?.name === 'NotFoundError' || error?.name === 'DevicesNotFoundError') {
        errorMessage = '–ú–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω –ø–æ–¥–∫–ª—é—á–µ–Ω.';
      } else if (error?.message) {
        errorMessage = `–û—à–∏–±–∫–∞: ${error.message}`;
      }
      
      setError(errorMessage);
    } finally {
      isStartingRef.current = false;
    }
  }, [connectWebSocket]); // Removed isMuted from deps

  /**
   * Stop voice call
   */
  const stopCall = useCallback(() => {
    try {
      // Send EOF to server
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ eof: 1 }));
      }

      // Stop microphone
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }

      // Disconnect worklet
      if (workletNodeRef.current) {
        workletNodeRef.current.disconnect();
        workletNodeRef.current = null;
      }

      // Close AudioContext for recording
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }

      // Close AudioContext for playback
      if (playbackAudioContextRef.current) {
        playbackAudioContextRef.current.close();
        playbackAudioContextRef.current = null;
      }

      // Clear audio queue
      audioQueueRef.current = [];
      isPlayingRef.current = false;

      setCallState('idle');
      setPartialTranscript('');
      console.log('‚úÖ Voice call stopped');
    } catch (error) {
      console.error('Error stopping call:', error);
    }
  }, []);

  /**
   * Cleanup on unmount
   */
  useEffect(() => {
    if (autoStart) {
      startCall();
    }
    return () => {
      stopCall();
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, [autoStart]); // eslint-disable-line react-hooks/exhaustive-deps

  return (
    <Card className={cn('p-4 space-y-4 bg-background/95 backdrop-blur border-primary/20 shadow-lg animate-in fade-in zoom-in duration-300', className)}>
      {/* Connection Status & Call State */}
      <div className="flex items-center justify-between">
        <div className="flex items-center gap-2">
          <div className={cn(
            'w-2 h-2 rounded-full',
            connectionState === 'connected' && 'animate-pulse',
            connectionState === 'connecting' && 'bg-yellow-500',
            connectionState === 'disconnected' && 'bg-gray-400',
            connectionState === 'error' && 'bg-red-500'
          )} style={connectionState === 'connected' ? { backgroundColor: '#1e983a' } : {}} />
          <span className="text-xs font-medium text-muted-foreground">
            {connectionState === 'connected' && 'AI –Ω–∞ —Å–≤—è–∑–∏'}
            {connectionState === 'connecting' && '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...'}
            {connectionState === 'disconnected' && '–û–∂–∏–¥–∞–Ω–∏–µ'}
            {connectionState === 'error' && '–û—à–∏–±–∫–∞'}
          </span>
        </div>

      </div>

      {/* Main Controls - simplified */}
      <div className="flex items-center justify-center gap-3">
        {isMediaDevicesSupported === false ? (
          <div className="w-full text-center p-4 bg-yellow-50 border border-yellow-200 rounded-lg">
            <p className="text-sm text-yellow-800">
              –ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ HTTP
            </p>
            <p className="text-xs text-yellow-600 mt-1">
              –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HTTPS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
            </p>
          </div>
        ) : callState === 'idle' ? (
          <Button
            size="lg"
            onClick={startCall}
            disabled={connectionState === 'connecting' || isMediaDevicesSupported === false}
            className="rounded-full w-full py-6 gap-3 text-lg font-semibold shadow-md hover:shadow-lg transition-all"
          >
            <Phone className="w-6 h-6" />
            –ù–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫
          </Button>
        ) : (
          <div className="flex items-center gap-3 w-full">
            <Button
              size="icon"
              variant="destructive"
              onClick={stopCall}
              className="rounded-full h-12 w-12 shrink-0 shadow-md hover:shadow-lg transition-all"
              title="–ó–∞–≤–µ—Ä—à–∏—Ç—å –∑–≤–æ–Ω–æ–∫"
            >
              <PhoneOff className="w-6 h-6" />
            </Button>

            <div className="flex-1 h-12 bg-secondary/50 rounded-full flex items-center px-4 overflow-hidden">
              <div className="flex-1 overflow-hidden">
                {callState === 'speaking' ? (
                  <div className="flex gap-1 items-center justify-center h-8">
                    {audioLevels.map((level, index) => {
                      // –í—ã—á–∏—Å–ª—è–µ–º –≤—ã—Å–æ—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω—è (–æ—Ç 8px –¥–æ 32px)
                      const height = 8 + level * 24;
                      // –ë–∞–∑–æ–≤—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
                      const delay = index * 0.1;
                      return (
                        <div
                          key={index}
                          className="w-1 bg-primary/70 rounded-full transition-all duration-100 ease-out"
                          style={{
                            height: `${height}px`,
                            minHeight: '8px',
                            maxHeight: '32px',
                            animationDelay: `${delay}s`,
                          }}
                        />
                      );
                    })}
                  </div>
                ) : (
                  <div className="flex gap-1 items-center justify-center opacity-30 h-8">
                    <div className="w-1 h-4 bg-primary/50 rounded-full" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full" />
                    <div className="w-1 h-8 bg-primary/50 rounded-full" />
                    <div className="w-1 h-6 bg-primary/50 rounded-full" />
                    <div className="w-1 h-4 bg-primary/50 rounded-full" />
                  </div>
                )}
              </div>
            </div>

            <Button
              size="icon"
              variant={isMuted ? 'secondary' : 'outline'}
              onClick={() => setIsMuted(!isMuted)}
              className={cn("rounded-full h-10 w-10 shrink-0 transition-all", isMuted && "bg-red-100 text-red-600 border-red-200")}
              title={isMuted ? '–í–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω' : '–í—ã–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω'}
            >
              {isMuted ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
            </Button>
            
            <Button
              size="icon"
              variant={audioEnabled ? 'outline' : 'secondary'}
              onClick={() => setAudioEnabled(!audioEnabled)}
              className="rounded-full h-10 w-10 shrink-0 transition-all"
              title={audioEnabled ? '–í—ã–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫' : '–í–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫'}
            >
              {audioEnabled ? <Volume2 className="w-5 h-5" /> : <VolumeX className="w-5 h-5" />}
            </Button>
          </div>
        )}
      </div>

      {error && (
        <div className="text-xs text-red-500 text-center bg-red-50 p-3 rounded border border-red-100 whitespace-pre-line">
          {error}
        </div>
      )}
    </Card>
  );
};

export default VoiceCall;
